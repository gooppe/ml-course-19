{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_data_file = \"./facebook_comment_volume/train.csv\"\n",
    "test_data_file = \"./facebook_comment_volume/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, lr=1e-4, max_iter=1000, batch_size=1024, shuffle=False, eps=1e-5):\n",
    "    def iter_data(x, y):\n",
    "        \"\"\"\n",
    "        Batchify dataset\n",
    "        \"\"\"\n",
    "        index = np.arange(len(x))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index)\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            span = slice(i, i + batch_size)\n",
    "            yield x[index[span]], y[index[span]]\n",
    "    \n",
    "    def grad_func(x, y, theta):\n",
    "        \"\"\"\n",
    "        Calculate gradient\n",
    "        \"\"\"\n",
    "        n, p = len(y), len(theta)\n",
    "        biased = np.ones((n, p))\n",
    "        biased[:, 1:] = x\n",
    "        y_ij = biased * theta\n",
    "        y_pred = y_ij.sum(-1)\n",
    "        diff = (y - y_pred)\n",
    "        mse = np.mean(diff ** 2)\n",
    "        grad = - 2 / n * diff @ biased\n",
    "        return grad, mse\n",
    "\n",
    "    y = y.reshape(-1)\n",
    "    param_size = x.shape[1] + 1\n",
    "    assert len(y) == x.shape[0], \"Features and target sizes mismatch.\"\n",
    "    \n",
    "    # Initialize parameters\n",
    "    theta = np.random.rand(param_size)\n",
    "    \n",
    "    prev_error = 1e100\n",
    "    pbar = tqdm(range(max_iter), total=max_iter)\n",
    "    batches = len(x) // batch_size + 1\n",
    "    for _ in pbar:\n",
    "        cum_loss = 0\n",
    "        for x_batch, y_batch in iter_data(x, y):\n",
    "            # Gradient descend\n",
    "            grad, error = grad_func(x_batch, y_batch, theta)\n",
    "            theta = theta - lr * grad\n",
    "            if np.linalg.norm(grad) < eps:\n",
    "                break\n",
    "            prev_error = error\n",
    "            cum_loss += error\n",
    "        pbar.set_description(f\"Loss: {cum_loss/batches:.4f}\")\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x, theta):\n",
    "    n, p = len(x), len(theta)\n",
    "    assert (n, p - 1) == x.shape, f\"Data and parameters sizes mismatch.\"\n",
    "    \n",
    "    biased = np.ones((n, p))\n",
    "    biased[:, 1:] = x\n",
    "    return biased @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y, y_pred):\n",
    "    mse = np.mean((y - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = 1 - mse / np.var(y)\n",
    "    return rmse, r2\n",
    "\n",
    "def linear_normalization(train, *args):\n",
    "    t_min = train.min(axis=0, keepdims=True)\n",
    "    t_max = train.max(axis=0, keepdims=True)\n",
    "    t_s = (t_max - t_min)\n",
    "    t_s[t_s == 0] = 1\n",
    "    return tuple((part - t_min) / t_s for part in [train, *args])\n",
    "\n",
    "def standard_score_normalization(train, *args):\n",
    "    mean = train.mean(axis=0, keepdims=True)\n",
    "    std = train.std(axis=0, keepdims=True)\n",
    "    std[std == 0] = 1\n",
    "    return tuple((part-mean)/std for part in [train, *args])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(x, y, a, b):\n",
    "    x = x.reshape(-1)\n",
    "    y = y.reshape(-1)\n",
    "\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, b * x + a, c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 1000/1000 [00:00<00:00, 1035.99it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 100\n",
    "x = np.linspace(-100, 10, n_samples).reshape(-1, 1)\n",
    "weights = np.array([3.])\n",
    "y = x @ weights + 3 * np.random.randn(n_samples)\n",
    "x, = linear_normalization(x)\n",
    "x, = standard_score_normalization(x)\n",
    "\n",
    "theta = train(x, y, 1e-1)\n",
    "a, b = theta\n",
    "\n",
    "plot_2d(x, y, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(x, k=5, shuffle=True):\n",
    "    index = np.arange(len(x))\n",
    "    if shuffle:\n",
    "        random.shuffle(index)\n",
    "    chunks = np.array_split(index, k)\n",
    "    for i in range(k):\n",
    "        train_index = np.concatenate(tuple(ch for j, ch in enumerate(chunks) if j != i))\n",
    "        test_index = chunks[i]\n",
    "        yield train_index, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with 5 folds on Facebook Comment Volume dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 857.5195: 100%|██████████| 1000/1000 [00:11<00:00, 86.07it/s]\n",
      "Loss: 868.6961: 100%|██████████| 1000/1000 [00:11<00:00, 86.52it/s]\n",
      "Loss: 825.3546: 100%|██████████| 1000/1000 [00:11<00:00, 84.76it/s]\n",
      "Loss: 866.3205: 100%|██████████| 1000/1000 [00:11<00:00, 83.43it/s]\n",
      "Loss: 846.2901: 100%|██████████| 1000/1000 [00:12<00:00, 83.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Test R2</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>p14</th>\n",
       "      <th>p15</th>\n",
       "      <th>p16</th>\n",
       "      <th>p17</th>\n",
       "      <th>p18</th>\n",
       "      <th>p19</th>\n",
       "      <th>p20</th>\n",
       "      <th>p21</th>\n",
       "      <th>p22</th>\n",
       "      <th>p23</th>\n",
       "      <th>p24</th>\n",
       "      <th>p25</th>\n",
       "      <th>p26</th>\n",
       "      <th>p27</th>\n",
       "      <th>p28</th>\n",
       "      <th>p29</th>\n",
       "      <th>p30</th>\n",
       "      <th>p31</th>\n",
       "      <th>p32</th>\n",
       "      <th>p33</th>\n",
       "      <th>p34</th>\n",
       "      <th>p35</th>\n",
       "      <th>p36</th>\n",
       "      <th>p37</th>\n",
       "      <th>p38</th>\n",
       "      <th>p39</th>\n",
       "      <th>p40</th>\n",
       "      <th>p41</th>\n",
       "      <th>p42</th>\n",
       "      <th>p43</th>\n",
       "      <th>p44</th>\n",
       "      <th>p45</th>\n",
       "      <th>p46</th>\n",
       "      <th>p47</th>\n",
       "      <th>p48</th>\n",
       "      <th>p49</th>\n",
       "      <th>p50</th>\n",
       "      <th>p51</th>\n",
       "      <th>p52</th>\n",
       "      <th>p53</th>\n",
       "      <th>p54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Fold 1</td>\n",
       "      <td>29.273844</td>\n",
       "      <td>29.150325</td>\n",
       "      <td>0.323562</td>\n",
       "      <td>0.310089</td>\n",
       "      <td>7.291322</td>\n",
       "      <td>0.289808</td>\n",
       "      <td>-0.515888</td>\n",
       "      <td>-1.760432</td>\n",
       "      <td>-0.121903</td>\n",
       "      <td>-0.911252</td>\n",
       "      <td>1.604900</td>\n",
       "      <td>0.823715</td>\n",
       "      <td>0.277827</td>\n",
       "      <td>3.722411</td>\n",
       "      <td>-1.420466</td>\n",
       "      <td>-0.847327</td>\n",
       "      <td>2.086890</td>\n",
       "      <td>2.972058</td>\n",
       "      <td>0.112946</td>\n",
       "      <td>-0.122953</td>\n",
       "      <td>1.513785</td>\n",
       "      <td>2.704796</td>\n",
       "      <td>3.967850</td>\n",
       "      <td>-1.392793</td>\n",
       "      <td>0.280635</td>\n",
       "      <td>-2.697869</td>\n",
       "      <td>-2.400529</td>\n",
       "      <td>-2.623685</td>\n",
       "      <td>0.183677</td>\n",
       "      <td>0.222116</td>\n",
       "      <td>-0.022573</td>\n",
       "      <td>0.887525</td>\n",
       "      <td>0.307373</td>\n",
       "      <td>-0.884366</td>\n",
       "      <td>0.569772</td>\n",
       "      <td>10.923749</td>\n",
       "      <td>3.821374</td>\n",
       "      <td>-2.922667</td>\n",
       "      <td>7.095880</td>\n",
       "      <td>-3.942319</td>\n",
       "      <td>-0.011955</td>\n",
       "      <td>2.467740</td>\n",
       "      <td>0.357315</td>\n",
       "      <td>0.860925</td>\n",
       "      <td>0.163650</td>\n",
       "      <td>0.059865</td>\n",
       "      <td>0.255926</td>\n",
       "      <td>0.706565</td>\n",
       "      <td>0.536355</td>\n",
       "      <td>0.679390</td>\n",
       "      <td>0.163961</td>\n",
       "      <td>0.253263</td>\n",
       "      <td>0.653099</td>\n",
       "      <td>0.435630</td>\n",
       "      <td>0.928870</td>\n",
       "      <td>0.394663</td>\n",
       "      <td>0.429821</td>\n",
       "      <td>0.178694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fold 2</td>\n",
       "      <td>29.460760</td>\n",
       "      <td>29.488864</td>\n",
       "      <td>0.326624</td>\n",
       "      <td>0.239492</td>\n",
       "      <td>7.331820</td>\n",
       "      <td>-1.742127</td>\n",
       "      <td>-0.525301</td>\n",
       "      <td>-0.789018</td>\n",
       "      <td>-0.085895</td>\n",
       "      <td>0.636554</td>\n",
       "      <td>1.866287</td>\n",
       "      <td>0.857517</td>\n",
       "      <td>0.917132</td>\n",
       "      <td>3.755649</td>\n",
       "      <td>-1.078185</td>\n",
       "      <td>-0.740077</td>\n",
       "      <td>1.512341</td>\n",
       "      <td>3.615352</td>\n",
       "      <td>-0.079977</td>\n",
       "      <td>0.055698</td>\n",
       "      <td>0.942047</td>\n",
       "      <td>1.564534</td>\n",
       "      <td>2.541456</td>\n",
       "      <td>-1.143989</td>\n",
       "      <td>-0.814225</td>\n",
       "      <td>-2.768764</td>\n",
       "      <td>-1.670747</td>\n",
       "      <td>-1.474508</td>\n",
       "      <td>-0.030491</td>\n",
       "      <td>-0.055250</td>\n",
       "      <td>0.721615</td>\n",
       "      <td>0.740846</td>\n",
       "      <td>-1.070785</td>\n",
       "      <td>-1.823872</td>\n",
       "      <td>-0.794381</td>\n",
       "      <td>10.720999</td>\n",
       "      <td>3.476699</td>\n",
       "      <td>-2.456859</td>\n",
       "      <td>6.840660</td>\n",
       "      <td>-4.019202</td>\n",
       "      <td>0.007457</td>\n",
       "      <td>4.431497</td>\n",
       "      <td>0.510149</td>\n",
       "      <td>0.705188</td>\n",
       "      <td>0.225551</td>\n",
       "      <td>0.216943</td>\n",
       "      <td>0.324824</td>\n",
       "      <td>0.597260</td>\n",
       "      <td>0.412777</td>\n",
       "      <td>0.540072</td>\n",
       "      <td>0.203918</td>\n",
       "      <td>0.324173</td>\n",
       "      <td>0.596991</td>\n",
       "      <td>0.435444</td>\n",
       "      <td>0.699162</td>\n",
       "      <td>0.390204</td>\n",
       "      <td>0.407405</td>\n",
       "      <td>0.281236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fold 3</td>\n",
       "      <td>28.718107</td>\n",
       "      <td>31.210252</td>\n",
       "      <td>0.332264</td>\n",
       "      <td>0.283062</td>\n",
       "      <td>7.279704</td>\n",
       "      <td>0.646715</td>\n",
       "      <td>-0.549239</td>\n",
       "      <td>-2.006348</td>\n",
       "      <td>-0.053280</td>\n",
       "      <td>0.844024</td>\n",
       "      <td>2.539500</td>\n",
       "      <td>1.856162</td>\n",
       "      <td>0.260061</td>\n",
       "      <td>4.619396</td>\n",
       "      <td>-2.374818</td>\n",
       "      <td>-0.476737</td>\n",
       "      <td>1.760746</td>\n",
       "      <td>4.439610</td>\n",
       "      <td>-1.596117</td>\n",
       "      <td>0.111685</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>1.694261</td>\n",
       "      <td>2.697079</td>\n",
       "      <td>-1.278650</td>\n",
       "      <td>-0.510991</td>\n",
       "      <td>-3.477171</td>\n",
       "      <td>-1.580369</td>\n",
       "      <td>-2.718637</td>\n",
       "      <td>-0.685557</td>\n",
       "      <td>-0.941752</td>\n",
       "      <td>0.433314</td>\n",
       "      <td>1.431163</td>\n",
       "      <td>-0.794189</td>\n",
       "      <td>-0.874792</td>\n",
       "      <td>-0.238444</td>\n",
       "      <td>11.137323</td>\n",
       "      <td>3.660935</td>\n",
       "      <td>-2.249833</td>\n",
       "      <td>7.051026</td>\n",
       "      <td>-3.906154</td>\n",
       "      <td>-0.022708</td>\n",
       "      <td>2.272880</td>\n",
       "      <td>0.065053</td>\n",
       "      <td>0.894068</td>\n",
       "      <td>0.185843</td>\n",
       "      <td>0.374278</td>\n",
       "      <td>0.375239</td>\n",
       "      <td>0.910157</td>\n",
       "      <td>0.568707</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.309178</td>\n",
       "      <td>0.501875</td>\n",
       "      <td>0.877443</td>\n",
       "      <td>0.418154</td>\n",
       "      <td>0.859466</td>\n",
       "      <td>0.341986</td>\n",
       "      <td>0.460712</td>\n",
       "      <td>0.377938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fold 4</td>\n",
       "      <td>29.426834</td>\n",
       "      <td>28.452214</td>\n",
       "      <td>0.321262</td>\n",
       "      <td>0.322831</td>\n",
       "      <td>7.477808</td>\n",
       "      <td>0.153739</td>\n",
       "      <td>-0.652697</td>\n",
       "      <td>-1.818991</td>\n",
       "      <td>-0.079189</td>\n",
       "      <td>0.045978</td>\n",
       "      <td>2.408939</td>\n",
       "      <td>3.071693</td>\n",
       "      <td>0.024854</td>\n",
       "      <td>4.510644</td>\n",
       "      <td>-2.069140</td>\n",
       "      <td>-0.253270</td>\n",
       "      <td>1.571337</td>\n",
       "      <td>3.223392</td>\n",
       "      <td>-1.243032</td>\n",
       "      <td>0.455070</td>\n",
       "      <td>1.502812</td>\n",
       "      <td>2.105834</td>\n",
       "      <td>1.991297</td>\n",
       "      <td>-1.130743</td>\n",
       "      <td>-0.569187</td>\n",
       "      <td>-3.107104</td>\n",
       "      <td>-0.612175</td>\n",
       "      <td>-2.222918</td>\n",
       "      <td>-0.562239</td>\n",
       "      <td>0.557937</td>\n",
       "      <td>0.351670</td>\n",
       "      <td>1.307595</td>\n",
       "      <td>-0.139662</td>\n",
       "      <td>-2.346415</td>\n",
       "      <td>0.656780</td>\n",
       "      <td>11.361897</td>\n",
       "      <td>3.175281</td>\n",
       "      <td>-2.880463</td>\n",
       "      <td>6.566913</td>\n",
       "      <td>-4.077232</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>2.476408</td>\n",
       "      <td>0.305917</td>\n",
       "      <td>0.894246</td>\n",
       "      <td>0.375218</td>\n",
       "      <td>0.293766</td>\n",
       "      <td>0.269303</td>\n",
       "      <td>0.832010</td>\n",
       "      <td>0.799222</td>\n",
       "      <td>0.867519</td>\n",
       "      <td>0.528456</td>\n",
       "      <td>0.197960</td>\n",
       "      <td>0.586990</td>\n",
       "      <td>0.539718</td>\n",
       "      <td>0.881103</td>\n",
       "      <td>0.593620</td>\n",
       "      <td>0.605741</td>\n",
       "      <td>0.217510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Fold 5</td>\n",
       "      <td>29.083284</td>\n",
       "      <td>29.996101</td>\n",
       "      <td>0.313670</td>\n",
       "      <td>0.342942</td>\n",
       "      <td>7.232730</td>\n",
       "      <td>0.438889</td>\n",
       "      <td>-0.299305</td>\n",
       "      <td>-2.233225</td>\n",
       "      <td>-0.052575</td>\n",
       "      <td>0.314571</td>\n",
       "      <td>1.954147</td>\n",
       "      <td>1.430125</td>\n",
       "      <td>1.417339</td>\n",
       "      <td>4.465436</td>\n",
       "      <td>-0.215983</td>\n",
       "      <td>0.260859</td>\n",
       "      <td>0.243549</td>\n",
       "      <td>3.715431</td>\n",
       "      <td>-0.897599</td>\n",
       "      <td>0.042124</td>\n",
       "      <td>2.127052</td>\n",
       "      <td>0.958264</td>\n",
       "      <td>3.069868</td>\n",
       "      <td>0.122439</td>\n",
       "      <td>-0.446053</td>\n",
       "      <td>-3.271096</td>\n",
       "      <td>-2.070988</td>\n",
       "      <td>-1.789177</td>\n",
       "      <td>-0.377328</td>\n",
       "      <td>0.979576</td>\n",
       "      <td>0.672919</td>\n",
       "      <td>0.884447</td>\n",
       "      <td>0.148818</td>\n",
       "      <td>-2.337721</td>\n",
       "      <td>0.748879</td>\n",
       "      <td>10.720626</td>\n",
       "      <td>3.675144</td>\n",
       "      <td>-3.149768</td>\n",
       "      <td>6.805543</td>\n",
       "      <td>-4.008036</td>\n",
       "      <td>0.090211</td>\n",
       "      <td>2.559361</td>\n",
       "      <td>0.124904</td>\n",
       "      <td>0.717858</td>\n",
       "      <td>0.297312</td>\n",
       "      <td>0.261936</td>\n",
       "      <td>0.180082</td>\n",
       "      <td>0.431215</td>\n",
       "      <td>0.561922</td>\n",
       "      <td>0.723927</td>\n",
       "      <td>0.242365</td>\n",
       "      <td>0.281745</td>\n",
       "      <td>0.709994</td>\n",
       "      <td>0.408426</td>\n",
       "      <td>0.780588</td>\n",
       "      <td>0.542115</td>\n",
       "      <td>0.617773</td>\n",
       "      <td>0.317760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mean</td>\n",
       "      <td>29.192566</td>\n",
       "      <td>29.659551</td>\n",
       "      <td>0.323476</td>\n",
       "      <td>0.299683</td>\n",
       "      <td>7.322677</td>\n",
       "      <td>-0.042595</td>\n",
       "      <td>-0.508486</td>\n",
       "      <td>-1.721603</td>\n",
       "      <td>-0.078568</td>\n",
       "      <td>0.185975</td>\n",
       "      <td>2.074754</td>\n",
       "      <td>1.607842</td>\n",
       "      <td>0.579443</td>\n",
       "      <td>4.214707</td>\n",
       "      <td>-1.431718</td>\n",
       "      <td>-0.411310</td>\n",
       "      <td>1.434973</td>\n",
       "      <td>3.593168</td>\n",
       "      <td>-0.740756</td>\n",
       "      <td>0.108325</td>\n",
       "      <td>1.268409</td>\n",
       "      <td>1.805538</td>\n",
       "      <td>2.853510</td>\n",
       "      <td>-0.964747</td>\n",
       "      <td>-0.411964</td>\n",
       "      <td>-3.064401</td>\n",
       "      <td>-1.666962</td>\n",
       "      <td>-2.165785</td>\n",
       "      <td>-0.294388</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>0.431389</td>\n",
       "      <td>1.050315</td>\n",
       "      <td>-0.309689</td>\n",
       "      <td>-1.653433</td>\n",
       "      <td>0.188521</td>\n",
       "      <td>10.972919</td>\n",
       "      <td>3.561887</td>\n",
       "      <td>-2.731918</td>\n",
       "      <td>6.872004</td>\n",
       "      <td>-3.990589</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>2.841577</td>\n",
       "      <td>0.272668</td>\n",
       "      <td>0.814457</td>\n",
       "      <td>0.249515</td>\n",
       "      <td>0.241357</td>\n",
       "      <td>0.281075</td>\n",
       "      <td>0.695441</td>\n",
       "      <td>0.575797</td>\n",
       "      <td>0.661424</td>\n",
       "      <td>0.289575</td>\n",
       "      <td>0.311803</td>\n",
       "      <td>0.684903</td>\n",
       "      <td>0.447474</td>\n",
       "      <td>0.829838</td>\n",
       "      <td>0.452518</td>\n",
       "      <td>0.504290</td>\n",
       "      <td>0.274628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Std</td>\n",
       "      <td>0.272202</td>\n",
       "      <td>0.923477</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.035818</td>\n",
       "      <td>0.083749</td>\n",
       "      <td>0.865381</td>\n",
       "      <td>0.115371</td>\n",
       "      <td>0.494680</td>\n",
       "      <td>0.025484</td>\n",
       "      <td>0.612551</td>\n",
       "      <td>0.348263</td>\n",
       "      <td>0.826311</td>\n",
       "      <td>0.513157</td>\n",
       "      <td>0.391742</td>\n",
       "      <td>0.761429</td>\n",
       "      <td>0.394783</td>\n",
       "      <td>0.628522</td>\n",
       "      <td>0.500883</td>\n",
       "      <td>0.659387</td>\n",
       "      <td>0.190202</td>\n",
       "      <td>0.629794</td>\n",
       "      <td>0.580893</td>\n",
       "      <td>0.656327</td>\n",
       "      <td>0.551981</td>\n",
       "      <td>0.368020</td>\n",
       "      <td>0.295524</td>\n",
       "      <td>0.603564</td>\n",
       "      <td>0.477133</td>\n",
       "      <td>0.325523</td>\n",
       "      <td>0.647121</td>\n",
       "      <td>0.266462</td>\n",
       "      <td>0.268709</td>\n",
       "      <td>0.535520</td>\n",
       "      <td>0.659586</td>\n",
       "      <td>0.604486</td>\n",
       "      <td>0.248139</td>\n",
       "      <td>0.222122</td>\n",
       "      <td>0.328972</td>\n",
       "      <td>0.190078</td>\n",
       "      <td>0.060149</td>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.800501</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.085011</td>\n",
       "      <td>0.077572</td>\n",
       "      <td>0.104282</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>0.169902</td>\n",
       "      <td>0.125134</td>\n",
       "      <td>0.133211</td>\n",
       "      <td>0.128681</td>\n",
       "      <td>0.103510</td>\n",
       "      <td>0.105857</td>\n",
       "      <td>0.047283</td>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.097350</td>\n",
       "      <td>0.089445</td>\n",
       "      <td>0.070771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Train RMSE  Test RMSE  Train R2   Test R2        p1        p2  \\\n",
       "Fold 1   29.273844  29.150325  0.323562  0.310089  7.291322  0.289808   \n",
       "Fold 2   29.460760  29.488864  0.326624  0.239492  7.331820 -1.742127   \n",
       "Fold 3   28.718107  31.210252  0.332264  0.283062  7.279704  0.646715   \n",
       "Fold 4   29.426834  28.452214  0.321262  0.322831  7.477808  0.153739   \n",
       "Fold 5   29.083284  29.996101  0.313670  0.342942  7.232730  0.438889   \n",
       "Mean     29.192566  29.659551  0.323476  0.299683  7.322677 -0.042595   \n",
       "Std       0.272202   0.923477  0.006135  0.035818  0.083749  0.865381   \n",
       "\n",
       "              p3        p4        p5        p6        p7        p8        p9  \\\n",
       "Fold 1 -0.515888 -1.760432 -0.121903 -0.911252  1.604900  0.823715  0.277827   \n",
       "Fold 2 -0.525301 -0.789018 -0.085895  0.636554  1.866287  0.857517  0.917132   \n",
       "Fold 3 -0.549239 -2.006348 -0.053280  0.844024  2.539500  1.856162  0.260061   \n",
       "Fold 4 -0.652697 -1.818991 -0.079189  0.045978  2.408939  3.071693  0.024854   \n",
       "Fold 5 -0.299305 -2.233225 -0.052575  0.314571  1.954147  1.430125  1.417339   \n",
       "Mean   -0.508486 -1.721603 -0.078568  0.185975  2.074754  1.607842  0.579443   \n",
       "Std     0.115371  0.494680  0.025484  0.612551  0.348263  0.826311  0.513157   \n",
       "\n",
       "             p10       p11       p12       p13       p14       p15       p16  \\\n",
       "Fold 1  3.722411 -1.420466 -0.847327  2.086890  2.972058  0.112946 -0.122953   \n",
       "Fold 2  3.755649 -1.078185 -0.740077  1.512341  3.615352 -0.079977  0.055698   \n",
       "Fold 3  4.619396 -2.374818 -0.476737  1.760746  4.439610 -1.596117  0.111685   \n",
       "Fold 4  4.510644 -2.069140 -0.253270  1.571337  3.223392 -1.243032  0.455070   \n",
       "Fold 5  4.465436 -0.215983  0.260859  0.243549  3.715431 -0.897599  0.042124   \n",
       "Mean    4.214707 -1.431718 -0.411310  1.434973  3.593168 -0.740756  0.108325   \n",
       "Std     0.391742  0.761429  0.394783  0.628522  0.500883  0.659387  0.190202   \n",
       "\n",
       "             p17       p18       p19       p20       p21       p22       p23  \\\n",
       "Fold 1  1.513785  2.704796  3.967850 -1.392793  0.280635 -2.697869 -2.400529   \n",
       "Fold 2  0.942047  1.564534  2.541456 -1.143989 -0.814225 -2.768764 -1.670747   \n",
       "Fold 3  0.256348  1.694261  2.697079 -1.278650 -0.510991 -3.477171 -1.580369   \n",
       "Fold 4  1.502812  2.105834  1.991297 -1.130743 -0.569187 -3.107104 -0.612175   \n",
       "Fold 5  2.127052  0.958264  3.069868  0.122439 -0.446053 -3.271096 -2.070988   \n",
       "Mean    1.268409  1.805538  2.853510 -0.964747 -0.411964 -3.064401 -1.666962   \n",
       "Std     0.629794  0.580893  0.656327  0.551981  0.368020  0.295524  0.603564   \n",
       "\n",
       "             p24       p25       p26       p27       p28       p29       p30  \\\n",
       "Fold 1 -2.623685  0.183677  0.222116 -0.022573  0.887525  0.307373 -0.884366   \n",
       "Fold 2 -1.474508 -0.030491 -0.055250  0.721615  0.740846 -1.070785 -1.823872   \n",
       "Fold 3 -2.718637 -0.685557 -0.941752  0.433314  1.431163 -0.794189 -0.874792   \n",
       "Fold 4 -2.222918 -0.562239  0.557937  0.351670  1.307595 -0.139662 -2.346415   \n",
       "Fold 5 -1.789177 -0.377328  0.979576  0.672919  0.884447  0.148818 -2.337721   \n",
       "Mean   -2.165785 -0.294388  0.152526  0.431389  1.050315 -0.309689 -1.653433   \n",
       "Std     0.477133  0.325523  0.647121  0.266462  0.268709  0.535520  0.659586   \n",
       "\n",
       "             p31        p32       p33       p34       p35       p36       p37  \\\n",
       "Fold 1  0.569772  10.923749  3.821374 -2.922667  7.095880 -3.942319 -0.011955   \n",
       "Fold 2 -0.794381  10.720999  3.476699 -2.456859  6.840660 -4.019202  0.007457   \n",
       "Fold 3 -0.238444  11.137323  3.660935 -2.249833  7.051026 -3.906154 -0.022708   \n",
       "Fold 4  0.656780  11.361897  3.175281 -2.880463  6.566913 -4.077232  0.039000   \n",
       "Fold 5  0.748879  10.720626  3.675144 -3.149768  6.805543 -4.008036  0.090211   \n",
       "Mean    0.188521  10.972919  3.561887 -2.731918  6.872004 -3.990589  0.020401   \n",
       "Std     0.604486   0.248139  0.222122  0.328972  0.190078  0.060149  0.040724   \n",
       "\n",
       "             p38       p39       p40       p41       p42       p43       p44  \\\n",
       "Fold 1  2.467740  0.357315  0.860925  0.163650  0.059865  0.255926  0.706565   \n",
       "Fold 2  4.431497  0.510149  0.705188  0.225551  0.216943  0.324824  0.597260   \n",
       "Fold 3  2.272880  0.065053  0.894068  0.185843  0.374278  0.375239  0.910157   \n",
       "Fold 4  2.476408  0.305917  0.894246  0.375218  0.293766  0.269303  0.832010   \n",
       "Fold 5  2.559361  0.124904  0.717858  0.297312  0.261936  0.180082  0.431215   \n",
       "Mean    2.841577  0.272668  0.814457  0.249515  0.241357  0.281075  0.695441   \n",
       "Std     0.800501  0.161000  0.085011  0.077572  0.104282  0.065957  0.169902   \n",
       "\n",
       "             p45       p46       p47       p48       p49       p50       p51  \\\n",
       "Fold 1  0.536355  0.679390  0.163961  0.253263  0.653099  0.435630  0.928870   \n",
       "Fold 2  0.412777  0.540072  0.203918  0.324173  0.596991  0.435444  0.699162   \n",
       "Fold 3  0.568707  0.496213  0.309178  0.501875  0.877443  0.418154  0.859466   \n",
       "Fold 4  0.799222  0.867519  0.528456  0.197960  0.586990  0.539718  0.881103   \n",
       "Fold 5  0.561922  0.723927  0.242365  0.281745  0.709994  0.408426  0.780588   \n",
       "Mean    0.575797  0.661424  0.289575  0.311803  0.684903  0.447474  0.829838   \n",
       "Std     0.125134  0.133211  0.128681  0.103510  0.105857  0.047283  0.081013   \n",
       "\n",
       "             p52       p53       p54  \n",
       "Fold 1  0.394663  0.429821  0.178694  \n",
       "Fold 2  0.390204  0.407405  0.281236  \n",
       "Fold 3  0.341986  0.460712  0.377938  \n",
       "Fold 4  0.593620  0.605741  0.217510  \n",
       "Fold 5  0.542115  0.617773  0.317760  \n",
       "Mean    0.452518  0.504290  0.274628  \n",
       "Std     0.097350  0.089445  0.070771  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(train_data_file, header=None)\n",
    "x = df.values[:,:-1]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "metric_log = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kfold(x)):\n",
    "    train_x, train_y = x[train_idx], y[train_idx]\n",
    "    test_x, test_y = x[test_idx], y[test_idx]\n",
    "    \n",
    "    train_x, test_x = standard_score_normalization(train_x, test_x)\n",
    "    \n",
    "    # Train model and make predictions for Train and Test parts.\n",
    "    theta = train(train_x, train_y, lr=0.001, shuffle=True)\n",
    "    train_pred_y = prediction(train_x, theta)\n",
    "    pred_y = prediction(test_x, theta)\n",
    "    \n",
    "    # Calculate metrics for Train and Test predictions.\n",
    "    train_rmse, train_r2 = metrics(train_y, train_pred_y)\n",
    "    rmse, r2 = metrics(test_y, pred_y)\n",
    "    \n",
    "    metric_log.append([train_rmse, rmse, train_r2, r2] + list(theta))\n",
    "\n",
    "mean = np.mean(metric_log, axis=0)\n",
    "std = np.std(metric_log, axis=0)\n",
    "\n",
    "columns = [\"Train RMSE\", \"Test RMSE\", \"Train R2\", \"Test R2\"] + [f\"p{p+1}\" for p in range(len(theta))]\n",
    "index = [f\"Fold {i+1}\" for i in range(len(metric_log))] + [\"Mean\"] + [\"Std\"]\n",
    "\n",
    "df = pd.DataFrame(np.array(metric_log + [mean] + [std]),\n",
    "                  columns=columns,\n",
    "                  index=index)\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
